---
title: "Data Processing with Tivy"
author: "Hans Ttito"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Data Processing with Tivy}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  warning = FALSE,
  message = FALSE
)
```

```{r setup}
library(Tivy)
library(dplyr)
library(ggplot2)
```

## Introduction

This vignette demonstrates how to process fisheries data from Peru's PRODUCE system using Tivy. We'll cover the complete workflow from raw data to analysis-ready datasets.

## Data Sources

Tivy is designed to work with data from PRODUCE's SITRAPESCA system, which includes:

1. **Fishing Hauls** (`calas`): Individual fishing operations
2. **Fishing Trips** (`faenas`): Vessel trips containing multiple hauls
3. **Length Frequencies** (`tallas`): Size structure data by haul

## Processing Fishing Hauls

### Basic Processing

```{r eval=FALSE}
# Load raw haul data
# In practice, you would load from Excel files:
# raw_hauls <- readxl::read_excel("path/to/calas_data.xlsx")

# Process hauls with automatic column detection
processed_hauls <- process_hauls(
  data_hauls = raw_hauls,
  correct_coordinates = TRUE,  # Automatically fix coordinate errors
  verbose = TRUE              # Show processing details
)

# View the structure
str(processed_hauls)
```

### Column Mapping

Tivy automatically detects columns using multiple patterns:

```{r eval=FALSE}
# The function looks for these patterns:
# - fishing_trip_code: "codigo.*faena", "trip.*code", "faena"
# - haul_number: "numero.*cala", "haul.*number", "cala"  
# - start_date: "fecha.*inicio", "start.*date", "inicio"
# - start_latitude: "latitud.*inicial", "lat.*inicial", "start.*lat"
# - species: "especie", "species", "sp"
# - catch: "captura", "catch", "peso", "kg"

# You can check what columns were found:
validate_haul_data(processed_hauls)
```

### Coordinate Processing

One of Tivy's key features is robust coordinate conversion:

```{r eval=FALSE}
# Convert various coordinate formats
coordinates <- c(
  "15°30'25\"S",     # Complete DMS
  "75°45'W",         # DM format
  "16 15 30 S",      # Space-separated
  "77°90'W"          # Invalid minutes (will be corrected)
)

decimal_coords <- dms_to_decimal(
  coordinates = coordinates,
  hemisphere = "S",           # Default hemisphere
  correct_errors = TRUE      # Fix invalid values
)

print(decimal_coords)
```

## Processing Fishing Trips

Fishing trips contain vessel and temporal information:

```{r eval=FALSE}
# Process fishing trips
processed_trips <- process_fishing_trips(
  data_fishing_trips = raw_trips,
  verbose = TRUE
)

# Key outputs include:
# - fishing_trip_code: Trip identifier
# - vessel: Vessel name  
# - id_vessel: Vessel registration
# - start_date_fishing_trip: Trip start
# - end_date_fishing_trip: Trip end
# - owner: Vessel owner (optional)

# Validate the processed data
trip_quality <- validate_fishing_trip_data(processed_trips)
print(trip_quality)
```

## Processing Length Frequency Data

Length data requires conversion from long to wide format:

```{r eval=FALSE}
# Process length frequency data
processed_lengths <- process_length(
  data_length = raw_length,
  verbose = TRUE
)

# This converts from:
# fishing_trip_code | haul_number | species | length | frequency
# To:
# fishing_trip_code | haul_number | species | 8 | 8.5 | 9 | 9.5 | ...

# Check the results
length_quality <- validate_length_data(processed_lengths)
print(length_quality)
```

## Data Integration

### Merging Datasets

Combine all three data sources:

```{r eval=FALSE}
# First merge length and trip data
length_trips <- merge(
  x = processed_lengths,
  y = processed_trips,
  by = "fishing_trip_code",
  all = TRUE
)

# Then merge with hauls data
complete_data <- merge_length_fishing_trips_hauls(
  data_hauls = processed_hauls,
  data_length_fishing_trips = length_trips
)

# This creates a comprehensive dataset with:
# - Spatial information (coordinates, distances)
# - Temporal information (dates, times)
# - Catch information (species, amounts)
# - Length structure (size frequencies)
# - Vessel information (name, owner, ID)
```

### Data Quality Assessment

Always validate your processed data:

```{r eval=FALSE}
# Individual dataset validation
haul_quality <- validate_haul_data(processed_hauls)
trip_quality <- validate_fishing_trip_data(processed_trips)
length_quality <- validate_length_data(processed_lengths)

# Print quality scores
cat("Data Quality Scores:\n")
cat("Hauls:", haul_quality$quality_score, "%\n")
cat("Trips:", trip_quality$quality_score, "%\n") 
cat("Lengths:", length_quality$quality_score, "%\n")

# Check for common issues
if (haul_quality$missing_coordinates > 0) {
  warning("Missing coordinates found in hauls data")
}

if (length_quality$duplicate_hauls > 0) {
  warning("Duplicate hauls found in length data")
}
```

## Advanced Processing

### Handling Different File Formats

```{r eval=FALSE}
# The package can handle various input formats
# Excel files (most common)
hauls_xlsx <- readxl::read_excel("data/calas.xlsx")

# CSV files
hauls_csv <- read.csv("data/calas.csv", stringsAsFactors = FALSE)

# All processing functions work the same way regardless of input format
processed_hauls <- process_hauls(hauls_xlsx)  # or hauls_csv
```

### Custom Column Mapping

If automatic detection fails, you can rename columns manually:

```{r eval=FALSE}
# Rename columns to standard patterns before processing
raw_data <- raw_data %>%
  rename(
    fishing_trip_code = "CodigoFaena",
    haul_number = "NumeroCala", 
    start_latitude = "LatitudInicial"
  )

# Then process normally
processed_data <- process_hauls(raw_data)
```

### Batch Processing

Process multiple files at once:

```{r eval=FALSE}
# Get list of files
haul_files <- list.files("data/hauls/", pattern = "\\.xlsx$", full.names = TRUE)

# Process all files
all_hauls <- list()
for (i in seq_along(haul_files)) {
  raw_data <- readxl::read_excel(haul_files[i])
  all_hauls[[i]] <- process_hauls(raw_data, verbose = FALSE)
}

# Combine all processed data
combined_hauls <- do.call(rbind, all_hauls)
```

## Error Handling

Tivy includes comprehensive error handling:

```{r eval=FALSE}
# Missing critical columns
tryCatch({
  processed_hauls <- process_hauls(incomplete_data)
}, error = function(e) {
  cat("Error:", e$message, "\n")
  # Handle the error appropriately
})

# Invalid coordinates
coordinates_with_errors <- c("15°30'S", "invalid_coord", "75°45'W")
clean_coords <- dms_to_decimal(
  coordinates_with_errors,
  correct_errors = TRUE  # This will handle invalid entries
)
```

## Tips for Best Results

### 1. Data Preparation
- Ensure column names are consistent across files
- Remove completely empty rows/columns
- Check for special characters in species names

### 2. Coordinate Quality
- Use `correct_errors = TRUE` for coordinate conversion
- Validate coordinates are within expected ranges for Peru
- Check for swapped latitude/longitude values

### 3. Date Handling
- Ensure dates are in recognizable formats
- Check timezone consistency
- Validate date ranges make sense

### 4. Species Names
- Standardize species names across datasets
- Handle both common and scientific names
- Watch for encoding issues with special characters

## Next Steps

After processing your data, you can:

1. **Add derived variables** with `add_variables()`
2. **Perform spatial analysis** (see spatial-analysis vignette)
3. **Analyze juvenile proportions** (see juvenile-analysis vignette)
4. **Create visualizations** with the plotting functions

```{r eval=FALSE}
# Example of next steps
enhanced_data <- add_variables(complete_data, JuvLim = 12)
dashboard <- create_fishery_dashboard(enhanced_data)
```